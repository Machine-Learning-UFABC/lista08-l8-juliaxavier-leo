---
title: "Solução Lista 08"
author: |
        | Nome: Julia Xavier
        | E-mail: julia.xavier@aluno.ufabc.edu.br
        | Nome: Leonardo Bernardes Lério
        | E-mail: leonardo.lerio@aluno.ufabc.edu.br
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T,
                      fig.align='center',
                      out.width = "60%",
                      out.heigth = "60%",
                      warning=FALSE,
                      message=FALSE)
options(width =70)

library(reticulate)
use_python("C:/Users/leonler/AppData/Local/Programs/Python/Python39/python.exe")
```

## Exercício 01
```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_diabetes
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import accuracy_score, precision_score, recall_score

diabetes = load_diabetes(as_frame=True)
data = diabetes.data
target = diabetes.target

df = pd.DataFrame(data, columns=diabetes.feature_names)

df['diabetes'] = target > target.mean()

X = df.drop('diabetes', axis=1)
y = df['diabetes']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = Sequential()
model.add(Dense(16, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(8, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=100, batch_size=5, validation_split=0.2, verbose=2)

y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

print(f"Acuracia: {accuracy:.4f}")
print(f"Precisao: {precision:.4f}")
print(f"Recall: {recall:.4f}")
```

## Exercício 02
```{R}
library(car)
library(tidyverse)
df <- as_tibble(Salaries)
write.csv(df, file = "Salaries.csv")
```

```{python}
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

data = pd.read_csv('Salaries.csv')
data

X = data.drop('salary', axis=1)
y = data['salary']

ohe = OneHotEncoder(drop='first', sparse=False)
X_encoded = ohe.fit_transform(X[['rank', 'discipline', 'sex']])
X_encoded = pd.DataFrame(X_encoded, columns=ohe.get_feature_names_out(['rank', 'discipline', 'sex']), index=X.index)
X_encoded = pd.concat([X.drop(['rank', 'discipline', 'sex'], axis=1), X_encoded], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='linear', input_shape=(X_encoded.shape[1],)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='linear'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='linear')
])

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=2)

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)
```


## Exercício 03
```{R}
library(keras)

# carregando o dataset mnist e convertendo os valores de pixels
# que são entre 0-255 para valores entre 0 e 1
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255

#################################
## 1. Define o modelo Keras
#################################
# A primeira camada deve especificar o argumento
# input_shape que representa as dimensões da entrada (28x28).
# Você deve completar o código adicionando:
# - uma camada densa (multilayer perceptron) com 128 neurônios e ativação relu
# - uma camada de dropout com taxa 0.2
# - uma camada de saída adequada
model <- keras_model_sequential() %>%
  layer_flatten(input_shape = c(28, 28)) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = 'softmax')

# Para checar seu modelo
summary(model)

#################################
## 2. Compilamos o modelo
#################################
# Compile aqui seu modelo. Utilize:
# - otimizador "adam",
# - função de perda "sparse_categorical_crossentropy"
# - métrica "accuracy"
model %>%
  compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = c('accuracy')
  )

##############################################
## 3. Ajustamos os dados ao conjunto de testes
##############################################
model %>%
  fit(
    x = mnist$train$x, y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 2
  )

#########################################################
## 4. Vamos testar o resultado usando o conjunto de testes
#########################################################
predictions <- predict(model, mnist$test$x)
head(predictions, 2)

model %>%
  evaluate(mnist$test$x, mnist$test$y, verbose = 0)

```